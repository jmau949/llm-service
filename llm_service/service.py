"""
LLM gRPC Service Implementation
------------------------------
Implementation of the LLM gRPC service.
"""

import time
import logging
import grpc
from concurrent import futures
from typing import Dict, Any

from llm_service.config import Config
from llm_service.utils.ollama import OllamaClient, LLMChunk

# Import generated protocol buffer code
# This will be generated by scripts/generate_proto.py
try:
    from proto import llm_pb2, llm_pb2_grpc
except ImportError:
    raise ImportError(
        "Protocol buffer code not found. "
        "Please run 'python scripts/generate_proto.py' first."
    )

logger = logging.getLogger(__name__)


class LLMService(llm_pb2_grpc.LLMServiceServicer):
    """
    gRPC service implementation for LLM inference.
    
    This service provides two endpoints:
    - GenerateStream: Streams the LLM response as it's generated
    - Generate: Returns the complete LLM response
    """
    
    def __init__(self, config: Config):
        """
        Initialize the LLM service with configuration.
        
        Args:
            config: Configuration object containing all settings
        """
        self.config = config
        self.ollama_client = OllamaClient(
            base_url=config.ollama_url,
            model_name=config.model_name,
            timeout=config.request_timeout
        )
        logger.info(
            f"Initialized LLM service with Ollama at {config.ollama_url} "
            f"using model {config.model_name}"
        )

    def GenerateStream(self, request, context):
        """
        Streams the LLM response back to the client.
        
        Args:
            request: The LLMRequest containing the prompt and parameters
            context: The gRPC context
            
        Yields:
            LLMResponse chunks as they are generated
        """
        logger.info(f"Received streaming request with prompt: {request.prompt[:50]}...")
        
        try:
            # Map gRPC request parameters to Ollama parameters
            ollama_params = self._map_parameters(request.parameters)
            
            # Stream responses from Ollama
            for chunk in self.ollama_client.generate_stream(request.prompt, ollama_params):
                yield llm_pb2.LLMResponse(
                    text=chunk.text,
                    is_complete=chunk.is_complete
                )
                
            logger.info("Streaming response completed")
            
        except Exception as e:
            logger.error(f"Error generating streaming response: {e}")
            context.abort(grpc.StatusCode.INTERNAL, f"Error generating response: {e}")

    def Generate(self, request, context):
        """
        Generates a complete LLM response.
        
        Args:
            request: The LLMRequest containing the prompt and parameters
            context: The gRPC context
            
        Returns:
            LLMCompleteResponse containing the full generated text
        """
        logger.info(f"Received non-streaming request with prompt: {request.prompt[:50]}...")
        
        try:
            # Map gRPC request parameters to Ollama parameters
            ollama_params = self._map_parameters(request.parameters)
            
            # Get complete response from Ollama
            response = self.ollama_client.generate(request.prompt, ollama_params)
            
            return llm_pb2.LLMCompleteResponse(text=response)
            
        except Exception as e:
            logger.error(f"Error generating complete response: {e}")
            context.abort(grpc.StatusCode.INTERNAL, f"Error generating response: {e}")

    def _map_parameters(self, params):
        """
        Map gRPC request parameters to Ollama API parameters.
        
        Args:
            params: Parameters from the gRPC request
            
        Returns:
            Dictionary of parameters for the Ollama API
        """
        # Default values from config if not specified
        temperature = params.temperature if params.temperature != 0 else self.config.default_temperature
        max_tokens = params.max_tokens if params.max_tokens != 0 else self.config.default_max_tokens
        top_p = params.top_p if params.top_p != 0 else self.config.default_top_p
        
        # Map to Ollama's parameter names
        return {
            "temperature": temperature,
            "top_p": top_p,
            "num_predict": max_tokens,
            # Add other parameters as needed
            "presence_penalty": params.presence_penalty,
            "frequency_penalty": params.frequency_penalty,
        }


def serve(config: Config):
    """
    Start the gRPC server with the given configuration.
    
    Args:
        config: Configuration object
    """
    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=config.worker_threads),
        options=[
            # Maximum message size (100MB)
            ('grpc.max_receive_message_length', 100 * 1024 * 1024),
            ('grpc.max_send_message_length', 100 * 1024 * 1024),
        ]
    )
    
    # Add the LLM service to the server
    llm_pb2_grpc.add_LLMServiceServicer_to_server(
        LLMService(config), server
    )
    
    # Add secure or insecure port based on configuration
    if config.use_tls:
        # Implementation for TLS would go here
        pass
    else:
        server.add_insecure_port(f'[::]:{config.port}')
    
    # Start the server
    server.start()
    logger.info(f"Server started, listening on port {config.port}")
    
    try:
        # Keep the server running
        while True:
            time.sleep(3600)  # One hour in seconds
    except KeyboardInterrupt:
        logger.info("Shutting down server...")
        server.stop(0)
        logger.info("Server stopped")